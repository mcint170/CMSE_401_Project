{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MariOh? - PyTorch meets Super Mario Bros\n",
    "\n",
    "By \"Brandon McIntyre\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Simple Icon of a camera. This is just a place holder for your image\" src=\"https://gamefabrique.com/storage/screenshots/nes/super-mario-bros-1-04.png\" width=\"20%\">\n",
    "\n",
    "Image from: [https://gamefabrique.com/](https://gamefabrique.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Abstract\n",
    "\n",
    "**Domain**  \n",
    "\n",
    "Reinforcement learning and games are no strangers. AlphaGo, a Deep-mind project by Google, uses reinforcement learning to play the classic strategy game of Go. AlphaGo is famous for the fact that it has now beaten the worlds greatest players of Go. Reinforcement learning works well for Go because the search space of Go is huge, we are talking 10^172 possible board positions. Reinforcement learning allows the machine to use trained Artificial Neural Networks to decide the next best move, allowing the machine to not be bogged down by all the possible board positions before making a move. The basic idea is that a neural network can be trained with rewards and punishments to complete a certain task. In the case of games this means to win. Games like Go are not the only games that machines learn using this type of learning. Video games like Super Mario Bros can also make use of this idea. Video games have enormous search spaces with screens and environments constantly changing with enemies and obstacles obstructing the player. This change has to be interpreted by the neural network, and actions such as jumping need to be timed with precision in order to not be taken out by enemy characters\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "My interest in this space is part childhood wonder, and part machine fascination. As a kid I grew up on video games. Always trying to beat the game and wondering how in the world some levels could ever be passed. Playing chess against a computer, and wondering how I always lost. I always knew computers could do things better than me and it felt as if they were run on magic. As I have gotten older and (hopefully) wiser I see computers do have their limit to what they can do, but at the same time that limit still dumbfounds me. Studying Data Science, with rudimentary knowledge in Cognitive Science, I am beginning to see how computers can do these magical things and it makes me even more giddy. I want to dive into this problem area to learn not only more about reinforcement learning, but how reinforcement learning can solve semi-complex tasks like playing a video game.\n",
    "\n",
    "<a id=comp></a>\n",
    "**Computation in Reinforcement Learning with video games**\n",
    "\n",
    "In order for machines to learn how to play video games with reinforcement learning, the game first has to be broken down into its basic components and feed into a neural network. These are things such as the world itself (characters and landscape), the scoring system of the game, and the controls that can be used to control the main character. Each component can then be mapped to a certain aspect of computation. The world itself can be reduced and modified to make it less data heavy, but still keep the important aspects of the world. This means removing un-important detail and perhaps also reducing the resolution of the world. This will then act as the direct input into the neural network, acting as what the model \"sees\". The scoring system of the game can be used to create the reward and punishment conditions. Whether that be coins, end states, character death, or other actions. This will act as the training aspect of the neural net. Finally, the controls of the games, aka what the main character does, can be mapped directly to the output layer of the neural network. This is how the neural network will interact with the game. The neural network will then have many hidden layers and weights that will act as the \"memory\" of the neural network that will work together to give the right output. The weights of the network will be updated by algorithms that incorporate the rewards and punishments from the scoring system. Of course underlying all this, is the actual simulation of the game itself.\n",
    "\n",
    "**Software and Hardware**\n",
    "\n",
    "The packagaes/software that will be used for this reinforcement learning will be `Pytorch`. This will act as the primary workhorse for the neural network. `Pytorch` has many functions from creating the neural net to training neural nets. The focus of the software is to provide deep-learning functions. By default `Pytorch` uses a processor to compute, but with special commands, it can run on GPUs with Cuda. `Gym` will also be used for simulating Super Mario Bros. `Gym` is a software that acts as a simulator. Also `conda` will be essential for running all the python scripts and other imports (like `numpy`, `random`, etc.). The code that utilizes this software comes from [this github repository](https://github.com/YuansongFeng/MadMario). It is apart of a [tutorial on PyTorch that showcases reinforcement learning](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html).  This will be run on MSU's High Performance Computer Cluster. Currently the program runs on one processor and one GPU. However, the HPCC will provide access to running multiple processors and GPUs, though I think most speed up will occur with multiple GPUs. \n",
    "\n",
    "**Benchmarking, Optimizing, and Defining Success**\n",
    "\n",
    "The statistic that will be benchmarked is the time it takes to run 40,000 epochs of the simulation. The 40,000 is because this is how many epochs the tutorial I am following suggested it will take for the computer to sufficiently understand the Super Mario Bros. Currently from my understanding of the code, the way this code can be sped up is using more than one GPU. Currently the code only asks for 1 GPU. A successful outcome will be if the neural network can complete world 1 of Super Mario Bros and the code runs faster than if it was run with only 1 GPU. Without testing I am unsure how fast it currently runs but the tutorial I am following has 10 epochs at 20 seconds. This means it would take 22 hours to run 40,000 epochs. My goal is to beat that time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Schedule\n",
    "Current Plan Week-by-week\n",
    "\n",
    "* Thursday February 11 - Project Proposal Milestone Due\n",
    "* Week of February 15 - Get code downloaded onto my computer and running\n",
    "* Week of February 22 - Get code running on my computer\n",
    "* Week of March 1 - Move Software onto HPCC and Try to get code working on HPCC\n",
    "* Week of March 8 - Try to get code working on HPCC\n",
    "* Week of March 15 - Write software abstract and Install instructions\n",
    "* Week of March 21 - Create Example Code \n",
    "* Week of March 22 - Create Example Code and Submission Script\n",
    "* Thursday March 25 - Project Part 1 Due\n",
    "* Week of March 28 - Parallelize code\n",
    "* Week of March 29 - Parallelize code\n",
    "* Week of April 5 - Perform timing Studies\n",
    "* Week of April 12 - Finish Methodology and Write Discussion/Abstract\n",
    "* Thursday April 15 - Final Project due"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 1 Software Exploration\n",
    "\n",
    "**PyTorch**\n",
    "\n",
    "[`PyTorch`](https://pytorch.org/) is a popular software/package for tensor computation and construction of Neural Networks. The software is [open source](https://github.com/pytorch/pytorch) and has a rich community that has a plethora of tutorials on how to use many of the features. One of `PyTorch`'s strengths is that it can utilize GPUs to perform calculations. This allows for significant speed up in training and computation. One thing that is interesting is the code is \"not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python.\" `PyTorch` also has many libraries such as [`torchaudio`](https://pytorch.org/audio/stable/index.html) for audio, [`torchtext`](https://pytorch.org/text/stable/index.html) for text, [`torchvision`](https://pytorch.org/vision/stable/index.html) for computer vision, [`TorchElastic`](https://pytorch.org/elastic/0.2.1/index.html) for running on changing environments, [`TorchServe`](https://pytorch.org/serve/) for serving `PyTorch` models. \n",
    "\n",
    "**Part 1 Code Example**\n",
    "\n",
    "The code example I will create for Part 1 of the project is most likely going to come from a tutorial on the `PyTorch` website. One tutorial I am expecting to complete soon is a tutorial on how to make a an [image classifier](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). Depending on how well that goes, I may use that as the code example. I am expecting to create a `getexample` type format, complete with Readme and submission script. Their program ran in 2 minutes 26.211 seconds, so I have belief this example will be underneath the 5 min threshold needed for Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 2 Benchmark and Optimization\n",
    "\n",
    "**Code**\n",
    "\n",
    "The code I wish to experiment with is located [here](https://github.com/yuansongFeng/MadMario/) with a tutorial on how to run the code [here](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html). The code uses Reinforcement Learning to teach a Neural Network to play Super Mario Bros. This approach uses [Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf) as the network structure. This network is built using smaller network structures available in `PyTorch`. The goal of the simulation is to teach the network how to win the first world of Super Mario Bros. `Gym` is used to simulate the Super Mario Bros game, and to create information to feed into the network. As discussed in [Computation in Reinforcement Learning with video games](#comp) there are a series of steps that need to be done in order to distill the information from the game to feed into the network. Things like grayscaling the image, reducing resolution, skipping frames, and other transformations. All these transformations use `PyTorch`'s `torchvision` library.\n",
    "\n",
    "**Benchmarking** \n",
    "\n",
    "As stated in the abstract, the goal is to run 40,000 epochs. Being that this could take up to 22 hours, at least according to the sample code, this is in need of a speed up. It appears that this is the time running on only 1 GPU. The way I hope to speed up this code is through using multiple GPUs. The documentation [here](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html) seems to suggest that this could be a one-liner, but I have a feeling it will be more complicated than that for this model. Perhaps I might even try my hand with this [feature](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html). Worst case, I do see a section of code where two different networks must be run, with no data shared between them, which looks like it could be a pleasantly parallel situation. Being that there are so many runs, even a small amount of time optimized can reduce time drastically. \n",
    "\n",
    "The code should be deemed successful if the network can make it to the end of the level. This can be checked with a test of the neural network and a real-time simulation of the game. \n",
    "\n",
    "**Optimization** \n",
    "\n",
    "With using multiple GPUs and possible `cuda` settings I hope to demonstrate a speed up of around at least 2x. I am not really sure how many GPUs the HPCC has to use, but if I could get access to 8, then I could possibly see a 4x to 8x speed up."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
